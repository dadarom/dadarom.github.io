<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
   <title>dadarom.github.io</title>
   <link href="http://dadarom.github.io/atom.xml" rel="self" type="application/atom+xml"/>
   <link href="http://dadarom.github.io" rel="alternate" type="text/html" />
   <updated>2014-12-25T21:55:34+08:00</updated>
   <id>http://dadarom.github.io</id>
   <author>
     <name></name>
     <email></email>
   </author>

   
   <entry>
     <title>Kafka 0.8.1：Monitoring--2</title>
     <link href="http://dadarom.github.com/Example-kafka-monitoring-config2"/>
     <updated>2014-12-09T00:00:00+08:00</updated>
     <id>http://ningg.github.com/Example-kafka-monitoring-config2</id>
     <content type="html">&lt;blockquote&gt;
  &lt;p&gt;本文原文来自[Kafka 0.8.x Monitoring][Kafka 0.8.x Monitoring]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Kafka uses &lt;code&gt;Yammer Metrics&lt;/code&gt; for metrics reporting in both the server and the client. This can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.&lt;/p&gt;

&lt;p&gt;The easiest way to see the available metrics to fire up jconsole and point it at a running kafka client or server; this will all browsing all metrics with JMX.&lt;/p&gt;

&lt;p&gt;We pay particular we do graphing and alerting on the following metrics:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Description&lt;/th&gt;
      &lt;th&gt;Mbean name&lt;/th&gt;
      &lt;th&gt;Normal value&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Message in rate&lt;/td&gt;
      &lt;td&gt;“kafka.server”: name=”AllTopicsMessagesInPerSec”, type=”BrokerTopicMetrics”&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Byte in rate&lt;/td&gt;
      &lt;td&gt;“kafka.server”: name=”AllTopicsBytesInPerSec”, type=”BrokerTopicMetrics”&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Request rate&lt;/td&gt;
      &lt;td&gt;“kafka.network”: name=”{Produce&lt;code&gt;|&lt;/code&gt;Fetch-consumer&lt;code&gt;|&lt;/code&gt;Fetch-follower}-RequestsPerSec”, type=”RequestMetrics”&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Byte out rate&lt;/td&gt;
      &lt;td&gt;“kafka.server”: name=”AllTopicsBytesOutPerSec”, type=”BrokerTopicMetrics”&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Log flush rate and time&lt;/td&gt;
      &lt;td&gt;“kafka.log”: name=”LogFlushRateAndTimeMs”, type=”LogFlushStats”&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;# of under replicated partitions&lt;/td&gt;
      &lt;td&gt;(&lt;code&gt;|ISR| &amp;lt; |all replicas|&lt;/code&gt;) “kafka.server”: name=”UnderReplicatedPartitions”, type=”ReplicaManager”&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Is controller active on broker&lt;/td&gt;
      &lt;td&gt;“kafka.controller”: name=”ActiveControllerCount”, type=”KafkaController”&lt;/td&gt;
      &lt;td&gt;only one broker in the cluster should have 1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Leader election rate&lt;/td&gt;
      &lt;td&gt;“kafka.controller”: name=”LeaderElectionRateAndTimeMs”, type=”ControllerStats”&lt;/td&gt;
      &lt;td&gt;non-zero when there are broker failures&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Unclean leader election rate&lt;/td&gt;
      &lt;td&gt;“kafka.controller”: name=”UncleanLeaderElectionsPerSec”, type=”ControllerStats”&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Partition counts&lt;/td&gt;
      &lt;td&gt;“kafka.server”: name=”PartitionCount”, type=”ReplicaManager”&lt;/td&gt;
      &lt;td&gt;mostly even across brokers&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Leader replica counts&lt;/td&gt;
      &lt;td&gt;“kafka.server”: name=”LeaderCount”, type=”ReplicaManager”&lt;/td&gt;
      &lt;td&gt;mostly even across brokers&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ISR shrink rate&lt;/td&gt;
      &lt;td&gt;“kafka.server”: name=”ISRShrinksPerSec”, type=”ReplicaManager”&lt;/td&gt;
      &lt;td&gt;If a broker goes down, ISR for some of the partitions will shrink. When that broker is up again, ISR will be expanded once the replicas are fully caught up. Other than that, the expected value for both ISR shrink rate and expansion rate is 0.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ISR expansion rate&lt;/td&gt;
      &lt;td&gt;“kafka.server”: name=”ISRExpandsPerSec”, type=”ReplicaManager”&lt;/td&gt;
      &lt;td&gt;See above&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Max lag in messages btw follower and leader replicas&lt;/td&gt;
      &lt;td&gt;“kafka.server”: name=”([-.\w]+)-MaxLag”, type=”ReplicaFetcherManager”&lt;/td&gt;
      &lt;td&gt;&amp;lt; replica.lag.max.messages&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Lag in messages per follower replica&lt;/td&gt;
      &lt;td&gt;“kafka.server”: name=”([-.\w]+)-ConsumerLag”, type=”FetcherLagMetrics”&lt;/td&gt;
      &lt;td&gt;&amp;lt; replica.lag.max.messages&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Requests waiting in the producer purgatory&lt;/td&gt;
      &lt;td&gt;“kafka.server”: name=”PurgatorySize”, type=”ProducerRequestPurgatory”&lt;/td&gt;
      &lt;td&gt;non-zero if ack=-1 is used&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Requests waiting in the fetch purgatory&lt;/td&gt;
      &lt;td&gt;“kafka.server”: name=”PurgatorySize”, type=”FetchRequestPurgatory”&lt;/td&gt;
      &lt;td&gt;size depends on fetch.wait.max.ms in the consumer&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Request total time&lt;/td&gt;
      &lt;td&gt;“kafka.network”: name=”{Produce&lt;code&gt;|&lt;/code&gt;Fetch-Consumer&lt;code&gt;|&lt;/code&gt;Fetch-Follower}-TotalTimeMs”,type=”RequestMetrics”&lt;/td&gt;
      &lt;td&gt;broken into queue, local, remote and response send time&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Time the request waiting in the request queue&lt;/td&gt;
      &lt;td&gt;“kafka.network”: name=”{Produce&lt;code&gt;|&lt;/code&gt;Fetch-Consumer&lt;code&gt;|&lt;/code&gt;Fetch-Follower}-QueueTimeMs”, type=”RequestMetrics”&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Time the request being processed at the leader&lt;/td&gt;
      &lt;td&gt;“kafka.network”: name=”{Produce&lt;code&gt;|&lt;/code&gt;Fetch-Consumer&lt;code&gt;|&lt;/code&gt;Fetch-Follower}-LocalTimeMs”, type=”RequestMetrics”&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Time the request waits for the follower&lt;/td&gt;
      &lt;td&gt;“kafka.network”: name=”{Produce&lt;code&gt;|&lt;/code&gt;Fetch-Consumer&lt;code&gt;|&lt;/code&gt;Fetch-Follower}-RemoteTimeMs”, type=”RequestMetrics”&lt;/td&gt;
      &lt;td&gt;non-zero for produce requests when ack=-1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Time to send the response&lt;/td&gt;
      &lt;td&gt;“kafka.network”: name=”{Produce&lt;code&gt;|&lt;/code&gt;Fetch-Consumer&lt;code&gt;|&lt;/code&gt;Fetch-Follower}-ResponseSendTimeMs”, type=”RequestMetrics”&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Number of messages the consumer lags behind the producer by&lt;/td&gt;
      &lt;td&gt;“kafka.consumer”: name=”([-.\w]+)-MaxLag”, type=”ConsumerFetcherManager”&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We recommend monitor GC time and other stats and various server stats such as CPU utilization, I/O service time, etc. On the client side, we recommend monitor the message/byte rate (global and per topic), request rate/size/time, and on the consumer side, max lag in messages among all partitions and min fetch request rate. For a consumer to keep up, max lag needs to be less than a threshold and min fetch rate needs to be larger than 0.
Audit&lt;/p&gt;

&lt;p&gt;The final alerting we do is on the correctness of the data delivery. We audit that every message that is sent is consumed by all consumers and measure the lag for this to occur. For important topics we alert if a certain completeness is not achieved in a certain time period. The details of this are discussed in KAFKA-260.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;note(ningg)&lt;/strong&gt;：Kafka中controller，MBean，两个名词的含义？&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://kafka.apache.org/documentation.html&quot;&gt;Kafka 0.8.* Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>Kafka 0.8.1：Monitoring</title>
     <link href="http://dadarom.github.com/Example-kafka-monitoring-config"/>
     <updated>2014-12-09T00:00:00+08:00</updated>
     <id>http://ningg.github.com/Example-kafka-monitoring-config</id>
     <content type="html">&lt;blockquote&gt;
  &lt;p&gt;本文原文来自[Kafka 0.8.x Monitoring][Kafka 0.8.x Monitoring]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Kafka uses &lt;code&gt;Yammer Metrics&lt;/code&gt; for metrics reporting in both the server and the client. This can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.&lt;/p&gt;

&lt;p&gt;The easiest way to see the available metrics to fire up jconsole and point it at a running kafka client or server; this will all browsing all metrics with JMX.&lt;/p&gt;

&lt;p&gt;We pay particular we do graphing and alerting on the following metrics:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Description&lt;/th&gt;
      &lt;th&gt;Mbean name&lt;/th&gt;
      &lt;th&gt;Normal value&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Message in rate&lt;/td&gt;
      &lt;td&gt;“kafka.server”: name=”AllTopicsMessagesInPerSec”, type=”BrokerTopicMetrics”&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Byte in rate&lt;/td&gt;
      &lt;td&gt;“kafka.server”: name=”AllTopicsBytesInPerSec”, type=”BrokerTopicMetrics”&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Request rate&lt;/td&gt;
      &lt;td&gt;“kafka.network”: name=”{Produce&lt;code&gt;|&lt;/code&gt;Fetch-consumer&lt;code&gt;|&lt;/code&gt;Fetch-follower}-RequestsPerSec”, type=”RequestMetrics”&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Byte out rate&lt;/td&gt;
      &lt;td&gt;“kafka.server”: name=”AllTopicsBytesOutPerSec”, type=”BrokerTopicMetrics”&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Log flush rate and time&lt;/td&gt;
      &lt;td&gt;“kafka.log”: name=”LogFlushRateAndTimeMs”, type=”LogFlushStats”&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;# of under replicated partitions&lt;/td&gt;
      &lt;td&gt;(&lt;code&gt;|ISR| &amp;lt; |all replicas|&lt;/code&gt;) “kafka.server”: name=”UnderReplicatedPartitions”, type=”ReplicaManager”&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Is controller active on broker&lt;/td&gt;
      &lt;td&gt;“kafka.controller”: name=”ActiveControllerCount”, type=”KafkaController”&lt;/td&gt;
      &lt;td&gt;only one broker in the cluster should have 1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Leader election rate&lt;/td&gt;
      &lt;td&gt;“kafka.controller”: name=”LeaderElectionRateAndTimeMs”, type=”ControllerStats”&lt;/td&gt;
      &lt;td&gt;non-zero when there are broker failures&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Unclean leader election rate&lt;/td&gt;
      &lt;td&gt;“kafka.controller”: name=”UncleanLeaderElectionsPerSec”, type=”ControllerStats”&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Partition counts&lt;/td&gt;
      &lt;td&gt;“kafka.server”: name=”PartitionCount”, type=”ReplicaManager”&lt;/td&gt;
      &lt;td&gt;mostly even across brokers&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Leader replica counts&lt;/td&gt;
      &lt;td&gt;“kafka.server”: name=”LeaderCount”, type=”ReplicaManager”&lt;/td&gt;
      &lt;td&gt;mostly even across brokers&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ISR shrink rate&lt;/td&gt;
      &lt;td&gt;“kafka.server”: name=”ISRShrinksPerSec”, type=”ReplicaManager”&lt;/td&gt;
      &lt;td&gt;If a broker goes down, ISR for some of the partitions will shrink. When that broker is up again, ISR will be expanded once the replicas are fully caught up. Other than that, the expected value for both ISR shrink rate and expansion rate is 0.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ISR expansion rate&lt;/td&gt;
      &lt;td&gt;“kafka.server”: name=”ISRExpandsPerSec”, type=”ReplicaManager”&lt;/td&gt;
      &lt;td&gt;See above&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Max lag in messages btw follower and leader replicas&lt;/td&gt;
      &lt;td&gt;“kafka.server”: name=”([-.\w]+)-MaxLag”, type=”ReplicaFetcherManager”&lt;/td&gt;
      &lt;td&gt;&amp;lt; replica.lag.max.messages&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Lag in messages per follower replica&lt;/td&gt;
      &lt;td&gt;“kafka.server”: name=”([-.\w]+)-ConsumerLag”, type=”FetcherLagMetrics”&lt;/td&gt;
      &lt;td&gt;&amp;lt; replica.lag.max.messages&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Requests waiting in the producer purgatory&lt;/td&gt;
      &lt;td&gt;“kafka.server”: name=”PurgatorySize”, type=”ProducerRequestPurgatory”&lt;/td&gt;
      &lt;td&gt;non-zero if ack=-1 is used&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Requests waiting in the fetch purgatory&lt;/td&gt;
      &lt;td&gt;“kafka.server”: name=”PurgatorySize”, type=”FetchRequestPurgatory”&lt;/td&gt;
      &lt;td&gt;size depends on fetch.wait.max.ms in the consumer&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Request total time&lt;/td&gt;
      &lt;td&gt;“kafka.network”: name=”{Produce&lt;code&gt;|&lt;/code&gt;Fetch-Consumer&lt;code&gt;|&lt;/code&gt;Fetch-Follower}-TotalTimeMs”,type=”RequestMetrics”&lt;/td&gt;
      &lt;td&gt;broken into queue, local, remote and response send time&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Time the request waiting in the request queue&lt;/td&gt;
      &lt;td&gt;“kafka.network”: name=”{Produce&lt;code&gt;|&lt;/code&gt;Fetch-Consumer&lt;code&gt;|&lt;/code&gt;Fetch-Follower}-QueueTimeMs”, type=”RequestMetrics”&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Time the request being processed at the leader&lt;/td&gt;
      &lt;td&gt;“kafka.network”: name=”{Produce&lt;code&gt;|&lt;/code&gt;Fetch-Consumer&lt;code&gt;|&lt;/code&gt;Fetch-Follower}-LocalTimeMs”, type=”RequestMetrics”&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Time the request waits for the follower&lt;/td&gt;
      &lt;td&gt;“kafka.network”: name=”{Produce&lt;code&gt;|&lt;/code&gt;Fetch-Consumer&lt;code&gt;|&lt;/code&gt;Fetch-Follower}-RemoteTimeMs”, type=”RequestMetrics”&lt;/td&gt;
      &lt;td&gt;non-zero for produce requests when ack=-1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Time to send the response&lt;/td&gt;
      &lt;td&gt;“kafka.network”: name=”{Produce&lt;code&gt;|&lt;/code&gt;Fetch-Consumer&lt;code&gt;|&lt;/code&gt;Fetch-Follower}-ResponseSendTimeMs”, type=”RequestMetrics”&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Number of messages the consumer lags behind the producer by&lt;/td&gt;
      &lt;td&gt;“kafka.consumer”: name=”([-.\w]+)-MaxLag”, type=”ConsumerFetcherManager”&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We recommend monitor GC time and other stats and various server stats such as CPU utilization, I/O service time, etc. On the client side, we recommend monitor the message/byte rate (global and per topic), request rate/size/time, and on the consumer side, max lag in messages among all partitions and min fetch request rate. For a consumer to keep up, max lag needs to be less than a threshold and min fetch rate needs to be larger than 0.
Audit&lt;/p&gt;

&lt;p&gt;The final alerting we do is on the correctness of the data delivery. We audit that every message that is sent is consumed by all consumers and measure the lag for this to occur. For important topics we alert if a certain completeness is not achieved in a certain time period. The details of this are discussed in KAFKA-260.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;note(ningg)&lt;/strong&gt;：Kafka中controller，MBean，两个名词的含义？&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;参考来源&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://kafka.apache.org/documentation.html&quot;&gt;Kafka 0.8.* Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
 
</feed>
